# Editorial: Farthest-in-Future Caching

## Introduction
Caching in computer system helps store small amounts of data in a fast memory  which helps in easy and fast access of data.

If the requested data is present in the cache it is called cache hit.

If the requested data is not present in the cache it is called cache miss.

When a cache miss occurs and the cache is full, an existing item must be evicted to make room for the new data.
## The Problem 
Suppose we have a cache $C$ which hold exactly k items.

$D = d_1, d_2, \dots, d_n$ are a sequence of memory references given to us.

- If $d_i\$ is already present in the cache it is a cache hit.
- If $d_i\$ is not present in the cache it is a cache miss and  $d_i\$ must be brought into cache .
- If cache is full and $d_i\$ is not present in the cache we will need to evict some other piece of data already present in the cache to make room for $d_i\$ .

**To Find:** The sequence of evictions that incurs as few cache misses as possible.

## The Intuition
We have a cache which holds 3 items= ${A,B,C}$

Suppose our cache is full and we must choose an item to evict:
- Item $A$ will be requested again in 2 steps.
- Item $B$ will be requested again in 10 steps.
- Item $C$ will never be requested again.

Evicting $A$ would be a bad choice as we would need it again after 2 steps.
Evicting $B$ would be better choice but not the best as we would need it again after 10 steps.
Evicting $C$ would be the best choice as it will need be never requested again.

When $d_i$ needs to be brought into the cache, evict the item that is needed the farthest into the future.We call this the **Farthest-in-Future Algorithm**.

## Pseudocode

```text
Algorithm Farthest-in-Future(k, D, n)
    Initialize an empty cache C
    miss_count ← 0

    For i = 1 to n:
        current_req ← D[i]
        
        If current_req is not in C:
            miss_count ← miss_count + 1
            
            If size of C < k:
                Add current_req to C
                
            Else:
                farthest_item ← null
                max_distance ← 0
                
                // Find the item to evict
                For each item e in C:
                    distance ← index of next appearance of e in D after step i
                    
                    If e never appears again:
                        distance ← infinity
                        
                    If distance > max_distance:
                        max_distance ← distance
                        farthest_item ← e
                
                Remove farthest_item from C
                Add current_req to C

    Return miss_count
```
## Proof of Optimality

As outlined in Kleinberg and Tardos, Farthest-in-Future is an optimal offline algorithm. We prove this using an exchange argument.

### The Exchange Argument

Let $S_{FF}$ be the schedule generated by Farthest-in-Future.
Let $S^*$ be an optimal schedule.

If the schedules are identical, $S_{FF}$ is optimal. If they differ, let step $i$ be the very first time they make different evictions during a cache miss:
- $S_{FF}$ swaps item $e$ (the item whose next request is farthest in the future).
- $S^*$ swaps item $f$.

We will construct a new, modified schedule called $S'$.
At step $i$, $S'$ will swap $e$ (matching $S_{FF}$) instead of $f$.

Immediately after step $i$, the caches of $S^*$ and $S'$ are almost identical, differing by only one item:
- $S^*$ contains $e$ but not $f$.
- $S'$ contains $f$ but not $e$.

For all subsequent steps $j > i$, $S'$ will simply copy the eviction decisions of $S^*$ until the two caches naturally converge back to being 100% identical.

Let's evaluate the future request sequence to see when this convergence happens and how it affects the miss count:

**Case 1: $S^*$ evicts $e$ before either $e$ or $f$ is requested**
- $S^*$ eventually decides to throw $e$ out.
- At this exact moment, $S'$ (which doesn't have $e$ anyway) chooses to throw $f$ out.
- The caches are now perfectly synchronized again. $S'$ incurred no extra misses compared to $S^*$.

**Case 2: The sequence requests $f$ before $e$**
- When $f$ is requested, $S^*$ suffers a cache miss because it evicted $f$ back at step $i$.
- However, $S'$ still has $f$ in its cache, so $S'$ gets a cache hit!
- To synchronize the caches, $S'$ can now safely evict $e$ to match whatever state $S^*$ is forced into.
- In this scenario, $S'$ actually had strictly fewer misses than the optimal schedule $S^*$.

**Case 3: The sequence requests $e$ before $f$**
- This case is mathematically impossible.
- By the definition of our greedy algorithm, $S_{FF}$ chose to evict $e$ at step $i$ specifically because its next request was strictly farther in the future than $f$. Therefore, $f$ must be requested before $e$.

### Conclusion

In all valid scenarios, we successfully created a schedule $S'$ that agrees with the Farthest-in-Future choice for one additional step, while guaranteeing:
$$\text{Misses}(S') \le \text{Misses}(S^*)$$

By repeatedly applying this exchange step at every point where the schedules diverge, we can completely transform the optimal schedule $S^*$ into $S_{FF}$ without ever increasing the miss count. Therefore, $S_{FF}$ is also an optimal schedule.



## Caching Strategies in Modern CPUs

### 1. Least Recently Used (LRU) and Why It’s Rarely Exact

The most straightforward hardware policy is LRU (Least Recently Used). This policy evicts the block that hasn’t been accessed for the longest time.

Why LRU?

It approximates First In, First Out (FiF) for temporal locality.

Programs tend to reuse data they accessed recently.

However, true LRU requires tracking the exact order of accesses within each set. For an $n$-way set, true LRU needs a complete ordering, which is costly in hardware.

As associativity increases (for example, 16-way), true LRU becomes too expensive.

So, CPUs rarely use true LRU.

### 2. Pseudo-LRU (PLRU)

Rather than using exact ordering, CPUs often use Pseudo-LRU.

A common method is tree-based PLRU.

This method maintains a small binary tree of bits for each set. Each bit shows which subtree was used more recently.

When evicting a block, you follow the bits to locate a “less recently used” candidate.

Advantages:

It requires only $n-1$ bits for an $n$-way set.

The hardware logic is very fast.

Tradeoff:

It does not always provide the true least recently used block.

But it is close enough for most workloads.

This is one of those engineering compromises: the decision quality may be slightly worse, but the circuitry is much simpler.

---
## Related Problems

LeetCode:

- LRU Cache : 
  https://leetcode.com/problems/lru-cache/

- LFU Cache :
  https://leetcode.com/problems/lfu-cache/

- Design Browser History (stack-like recency behavior) :
  https://leetcode.com/problems/design-browser-history/

- Cache With Time Limit :
  https://leetcode.com/problems/cache-with-time-limit/

Codeforces:

- Little Elephant and Array (cache-style simulation flavor) :
  https://codeforces.com/problemset/problem/220/B

- Valera and Queries (offline processing idea — useful for understanding future knowledge tricks) :
  https://codeforces.com/problemset/problem/369/E
---
